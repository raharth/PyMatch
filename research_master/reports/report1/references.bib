
@article{van_hasselt_deep_2015,
	title = {Deep Reinforcement Learning with Double Q-Learning},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions afﬁrmatively. In particular, we ﬁrst show that the recent {DQN} algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a speciﬁc adaptation to the {DQN} algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	pages = {7},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	date = {2015},
	langid = {english},
	keywords = {Q-Learning, read},
	file = {van Hasselt et al. - Deep Reinforcement Learning with Double Q-Learning.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\BX94I4G7\\van Hasselt et al. - Deep Reinforcement Learning with Double Q-Learning.pdf:application/pdf}
}

@article{mnih_asynchronous_2016,
	title = {Asynchronous Methods for Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core {CPU} instead of a {GPU}. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	journaltitle = {{arXiv}:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	urldate = {2018-10-08},
	date = {2016-02-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1602.01783},
	keywords = {Reinforcement Learning, skimed, Deep Learning},
	file = {Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\76AKICNY\\Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf}
}

@article{konda_actor-critic_2000,
	title = {Actor-Critic Algorithms},
	abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses {TD} learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
	pages = {7},
	journaltitle = {{NIPS}},
	author = {Konda, Vijay R and Tsitsiklis, John N},
	date = {2000},
	langid = {english},
	keywords = {Policy Gradient, Reinforcement Learning, skimed, Actor-Critics},
	file = {Konda und Tsitsiklis - Actor-Critic Algorithms.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\UK2WMTEM\\Konda und Tsitsiklis - Actor-Critic Algorithms.pdf:application/pdf}
}

@article{pearce_uncertainty_2018,
	title = {Uncertainty in Neural Networks: Bayesian Ensembling},
	url = {http://arxiv.org/abs/1810.05546},
	shorttitle = {Uncertainty in Neural Networks},
	abstract = {Understanding the uncertainty of a neural network’s ({NN}) predictions is essential for many applications. The Bayesian framework provides a principled approach to this, however applying it to {NNs} is challenging due to the large number of parameters and data. Ensembling {NNs} provides an easily implementable, scalable method for uncertainty quantiﬁcation, however, it has been criticised for not being Bayesian. In this work we propose one modiﬁcation to the usual ensembling process that does result in Bayesian behaviour: regularising parameters about values drawn from a prior distribution. We provide theoretical support for this procedure as well as empirical evaluations on regression, image classiﬁcation, and reinforcement learning problems.},
	journaltitle = {{arXiv}:1810.05546 [cs, stat]},
	author = {Pearce, Tim and Zaki, Mohamed and Brintrup, Alexandra and Anastassacos, Nicolas and Neely, Andy},
	urldate = {2019-07-22},
	date = {2018-10-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.05546},
	keywords = {Deep Learning, read, Neural Network, Uncertainty, Bayesian, Ensemble},
	file = {Pearce et al. - 2018 - Uncertainty in Neural Networks Bayesian Ensemblin.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\T383MBTN\\Pearce et al. - 2018 - Uncertainty in Neural Networks Bayesian Ensemblin.pdf:application/pdf}
}

@misc{williams_simple_1992,
	title = {Simple Statistical Gradient-Following Algorithm for Connectionist Reinforcement Learning},
	author = {Williams, Ronald},
	date = {1992},
	keywords = {Reinforcement Learning},
	file = {williams92simple.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\5TX7CNWN\\williams92simple.pdf:application/pdf}
}

@book{sutton_introduction_1998,
	location = {Cambridge, {MA}, {USA}},
	edition = {1st},
	title = {Introduction to Reinforcement Learning},
	isbn = {0-262-19398-1},
	publisher = {{MIT} Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	date = {1998},
	keywords = {read}
}

@article{clements_estimating_2020,
	title = {Estimating Risk and Uncertainty in Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1905.09638},
	abstract = {We propose a method for disentangling epistemic and aleatoric uncertainties in deep reinforcement learning. Aleatoric uncertainty, or risk, which arises from inherently stochastic environments or agents, must be accounted for in the design of risk-sensitive algorithms. Epistemic uncertainty, which stems from limited data, is important both for risk-sensitivity and for efficient exploration. Our method combines elements from distributional reinforcement learning and approximate Bayesian inference techniques with neural networks, allowing us to disentangle both types of uncertainty on the expected return of a policy. Specifically, the learned return distribution provides the aleatoric uncertainty, and the Bayesian posterior yields the epistemic uncertainty. Although our approach in principle requires a large number of samples from the Bayesian posterior to estimate the epistemic uncertainty, we show that two networks already yield a useful approximation. We perform experiments that illustrate our method and some applications.},
	journaltitle = {{arXiv}:1905.09638 [cs, stat]},
	author = {Clements, William R. and Robaglia, Benoît-Marie and Van Delft, Bastien and Slaoui, Reda Bahi and Toth, Sébastien},
	urldate = {2020-07-12},
	date = {2020-02-17},
	eprinttype = {arxiv},
	eprint = {1905.09638},
	keywords = {Deep Learning, read, Uncertainty, Epistemic Uncertainty, Aleatoric Uncertainty},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jonas\\Zotero\\storage\\JVRFURCS\\Clements et al. - 2020 - Estimating Risk and Uncertainty in Deep Reinforcem.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jonas\\Zotero\\storage\\42DK5FZK\\1905.html:text/html}
}

@article{da_silva_uncertainty-aware_2020,
	title = {Uncertainty-Aware Action Advising for Deep Reinforcement Learning Agents},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/6036},
	doi = {10.1609/aaai.v34i04.6036},
	abstract = {Although Reinforcement Learning ({RL}) has been one of the most successful approaches for learning in sequential decision making problems, the sample-complexity of {RL} techniques still represents a major challenge for practical applications. To combat this challenge, whenever a competent policy (e.g., either a legacy system or a human demonstrator) is available, the agent could leverage samples from this policy (advice) to improve sample-efﬁciency. However, advice is normally limited, hence it should ideally be directed to states where the agent is uncertain on the best action to execute. In this work, we propose Requesting Conﬁdence-Moderated Policy advice ({RCMP}), an action-advising framework where the agent asks for advice when its epistemic uncertainty is high for a certain state. {RCMP} takes into account that the advice is limited and might be suboptimal. We also describe a technique to estimate the agent uncertainty by performing minor modiﬁcations in standard value-function-based {RL} methods. Our empirical evaluations show that {RCMP} performs better than Importance Advising, not receiving advice, and receiving it at random states in Gridworld and Atari Pong scenarios.},
	pages = {5792--5799},
	number = {4},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Da Silva, Felipe Leno and Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E.},
	urldate = {2020-07-12},
	date = {2020-04-03},
	langid = {english},
	keywords = {Reinforcement Learning, read, Uncertainty, Confidence, Epistemic Uncertainty, Advice},
	file = {Da Silva et al. - 2020 - Uncertainty-Aware Action Advising for Deep Reinfor.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\HI3FLWPJ\\Da Silva et al. - 2020 - Uncertainty-Aware Action Advising for Deep Reinfor.pdf:application/pdf}
}

@article{gal_dropout_2016,
	title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
	url = {http://arxiv.org/abs/1506.02142},
	shorttitle = {Dropout as a Bayesian Approximation},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classiﬁcation do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks ({NNs}) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout {NNs} –extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacriﬁcing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classiﬁcation, using {MNIST} as an example. We show a considerable improvement in predictive log-likelihood and {RMSE} compared to existing state-of-the-art methods, and ﬁnish by using dropout’s uncertainty in deep reinforcement learning.},
	journaltitle = {{arXiv}:1506.02142 [cs, stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	urldate = {2020-06-17},
	date = {2016-10-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.02142},
	keywords = {read, Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf:C\:\\Users\\Jonas\\Zotero\\storage\\EA3RIIXY\\Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf:application/pdf;Appendix:C\:\\Users\\Jonas\\Zotero\\storage\\A82I2Z63\\Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf:application/pdf}
}

@article{loquercio_general_2020,
	title = {A General Framework for Uncertainty Estimation in Deep Learning},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/9001195/},
	doi = {10.1109/LRA.2020.2974682},
	abstract = {Neural networks predictions are unreliable when the input sample is out of the training distribution or corrupted by noise. Being able to detect such failures automatically is fundamental to integrate deep learning algorithms into robotics. Current approaches for uncertainty estimation of neural networks require changes to the network and optimization process, typically ignore prior knowledge about the data, and tend to make over-simplifying assumptions which underestimate uncertainty. To address these limitations, we propose a novel framework for uncertainty estimation. Based on Bayesian belief networks and Monte-Carlo sampling, our framework not only fully models the different sources of prediction uncertainty, but also incorporates prior data information, e.g. sensor noise. We show theoretically that this gives us the ability to capture uncertainty better than existing methods. In addition, our framework has several desirable properties: (i) it is agnostic to the network architecture and task; (ii) it does not require changes in the optimization process; (iii) it can be applied to already trained architectures. We thoroughly validate the proposed framework through extensive experiments on both computer vision and control tasks, where we outperform previous methods by up to 23\% in accuracy. The video available at https://youtu.be/X7n-{bRS}5vSM shows qualitative results of our experiments. The project’s code is available at: https://tinyurl.com/s3nygw7.},
	pages = {3153--3160},
	number = {2},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	shortjournal = {{IEEE} Robot. Autom. Lett.},
	author = {Loquercio, Antonio and Segu, Mattia and Scaramuzza, Davide},
	urldate = {2020-09-17},
	date = {2020-04},
	langid = {english},
	keywords = {read},
	file = {Loquercio et al. - 2020 - A General Framework for Uncertainty Estimation in .pdf:C\:\\Users\\Jonas\\Zotero\\storage\\DRZA76UW\\Loquercio et al. - 2020 - A General Framework for Uncertainty Estimation in .pdf:application/pdf}
}

@article{osband_deep_2016,
	title = {Deep Exploration via Bootstrapped {DQN}},
	url = {http://arxiv.org/abs/1602.04621},
	abstract = {Eﬃcient exploration remains a major challenge for reinforcement learning ({RL}). Common dithering strategies for exploration, such as -greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically eﬃcient {RL} are not computationally tractable in complex environments. Randomized value functions oﬀer a promising approach to eﬃcient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a ﬁrst step towards addressing such contexts we develop bootstrapped {DQN}. We demonstrate that bootstrapped {DQN} can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped {DQN} substantially improves learning speed and cumulative performance across most games.},
	journaltitle = {{arXiv}:1602.04621 [cs, stat]},
	author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	urldate = {2020-09-26},
	date = {2016-07-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1602.04621},
	keywords = {read, Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Systems and Control},
	file = {Osband et al. - 2016 - Deep Exploration via Bootstrapped DQN.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\5Z9Q9QFY\\Osband et al. - 2016 - Deep Exploration via Bootstrapped DQN.pdf:application/pdf}
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	pages = {529--533},
	number = {7540},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	urldate = {2020-10-23},
	date = {2015-02},
	langid = {english},
	keywords = {read},
	file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\D3FITC3U\\Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf}
}

@online{openai_gym_nodate,
	title = {Gym: A toolkit for developing and comparing reinforcement learning algorithms},
	url = {https://gym.openai.com},
	shorttitle = {Gym},
	abstract = {A toolkit for developing and comparing reinforcement learning algorithms},
	author = {{OpenAI}},
	urldate = {2020-11-17},
	file = {Snapshot:C\:\\Users\\Jonas\\Zotero\\storage\\DR8DN6XW\\gym.openai.com.html:text/html}
}

@article{silver_general_2018,
	title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	volume = {362},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aar6404},
	doi = {10.1126/science.aar6404},
	abstract = {The game of chess is the longest-studied domain in the history of artiﬁcial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-speciﬁc adaptations, and handcrafted evaluation functions that have been reﬁned by human experts over several decades. By contrast, the {AlphaGo} Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from selfplay. In this paper, we generalize this approach into a single {AlphaZero} algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, {AlphaZero} convincingly defeated a world champion program in the games of chess and shogi (Japanese chess) as well as Go.},
	pages = {1140--1144},
	number = {6419},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	urldate = {2020-11-19},
	date = {2018-12-07},
	langid = {english},
	keywords = {Reinforcement Learning, Deep Learning, Actor-Critics, read},
	file = {Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\CRPRELW2\\Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:application/pdf}
}

@article{bicchi_safety_nodate,
	title = {Safety for Ph 57. Safety for Physical Human–Robot Interaction},
	pages = {14},
	author = {Bicchi, Antonio and Peshkin, Michael A and Colgate, J Edward},
	langid = {english},
	keywords = {Robotics, read},
	file = {Bicchi et al. - Safety for Ph 57. Safety for Physical Human–Robot .pdf:C\:\\Users\\Jonas\\Zotero\\storage\\5E3435C2\\Bicchi et al. - Safety for Ph 57. Safety for Physical Human–Robot .pdf:application/pdf}
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	pages = {350--354},
	number = {7782},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and {McKinney}, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	urldate = {2020-11-19},
	date = {2019-11-14},
	langid = {english},
	file = {Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\QCTVU2YX\\Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf:application/pdf}
}

@online{noauthor_weights_nodate,
	title = {Weights \& Biases – Developer tools for {ML}},
	url = {https://www.wandb.com/},
	abstract = {A central dashboard to keep track of your hyperparameters, system metrics, and predictions so you can compare models live, and share your findings.},
	urldate = {2020-11-20},
	file = {Snapshot:C\:\\Users\\Jonas\\Zotero\\storage\\K4SXZU8D\\www.wandb.com.html:text/html}
}