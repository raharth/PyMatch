
@article{van_hasselt_deep_2015,
	title = {Deep Reinforcement Learning with Double Q-Learning},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions afﬁrmatively. In particular, we ﬁrst show that the recent {DQN} algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a speciﬁc adaptation to the {DQN} algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	pages = {7},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	date = {2015},
	langid = {english},
	keywords = {Q-Learning, read},
	file = {van Hasselt et al. - Deep Reinforcement Learning with Double Q-Learning.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\BX94I4G7\\van Hasselt et al. - Deep Reinforcement Learning with Double Q-Learning.pdf:application/pdf}
}

@article{mnih_asynchronous_2016,
	title = {Asynchronous Methods for Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core {CPU} instead of a {GPU}. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	journaltitle = {{arXiv}:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	urldate = {2018-10-08},
	date = {2016-02-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1602.01783},
	keywords = {Reinforcement Learning, skimed, Deep Learning},
	file = {Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\76AKICNY\\Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf}
}

@article{konda_actor-critic_2000,
	title = {Actor-Critic Algorithms},
	abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses {TD} learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
	pages = {7},
	journaltitle = {{NIPS}},
	author = {Konda, Vijay R and Tsitsiklis, John N},
	date = {2000},
	langid = {english},
	keywords = {Policy Gradient, Reinforcement Learning, skimed, Actor-Critics},
	file = {Konda und Tsitsiklis - Actor-Critic Algorithms.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\UK2WMTEM\\Konda und Tsitsiklis - Actor-Critic Algorithms.pdf:application/pdf}
}

@misc{williams_simple_1992,
	title = {Simple Statistical Gradient-Following Algorithm for Connectionist Reinforcement Learning},
	author = {Williams, Ronald},
	date = {1992},
	keywords = {Reinforcement Learning},
	file = {williams92simple.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\5TX7CNWN\\williams92simple.pdf:application/pdf}
}

@book{sutton_introduction_1998,
	location = {Cambridge, {MA}, {USA}},
	edition = {1st},
	title = {Introduction to Reinforcement Learning},
	isbn = {0-262-19398-1},
	publisher = {{MIT} Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	date = {1998},
	keywords = {read}
}

@article{gal_dropout_2016,
	title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
	url = {http://arxiv.org/abs/1506.02142},
	shorttitle = {Dropout as a Bayesian Approximation},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classiﬁcation do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks ({NNs}) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout {NNs} –extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacriﬁcing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classiﬁcation, using {MNIST} as an example. We show a considerable improvement in predictive log-likelihood and {RMSE} compared to existing state-of-the-art methods, and ﬁnish by using dropout’s uncertainty in deep reinforcement learning.},
	journaltitle = {{arXiv}:1506.02142 [cs, stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	urldate = {2020-06-17},
	date = {2016-10-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.02142},
	keywords = {read, Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf:C\:\\Users\\Jonas\\Zotero\\storage\\EA3RIIXY\\Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf:application/pdf;Appendix:C\:\\Users\\Jonas\\Zotero\\storage\\A82I2Z63\\Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf:application/pdf}
}

@article{loquercio_general_2020,
	title = {A General Framework for Uncertainty Estimation in Deep Learning},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/9001195/},
	doi = {10.1109/LRA.2020.2974682},
	abstract = {Neural networks predictions are unreliable when the input sample is out of the training distribution or corrupted by noise. Being able to detect such failures automatically is fundamental to integrate deep learning algorithms into robotics. Current approaches for uncertainty estimation of neural networks require changes to the network and optimization process, typically ignore prior knowledge about the data, and tend to make over-simplifying assumptions which underestimate uncertainty. To address these limitations, we propose a novel framework for uncertainty estimation. Based on Bayesian belief networks and Monte-Carlo sampling, our framework not only fully models the different sources of prediction uncertainty, but also incorporates prior data information, e.g. sensor noise. We show theoretically that this gives us the ability to capture uncertainty better than existing methods. In addition, our framework has several desirable properties: (i) it is agnostic to the network architecture and task; (ii) it does not require changes in the optimization process; (iii) it can be applied to already trained architectures. We thoroughly validate the proposed framework through extensive experiments on both computer vision and control tasks, where we outperform previous methods by up to 23\% in accuracy. The video available at https://youtu.be/X7n-{bRS}5vSM shows qualitative results of our experiments. The project’s code is available at: https://tinyurl.com/s3nygw7.},
	pages = {3153--3160},
	number = {2},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	shortjournal = {{IEEE} Robot. Autom. Lett.},
	author = {Loquercio, Antonio and Segu, Mattia and Scaramuzza, Davide},
	urldate = {2020-09-17},
	date = {2020-04},
	langid = {english},
	keywords = {read},
	file = {Loquercio et al. - 2020 - A General Framework for Uncertainty Estimation in .pdf:C\:\\Users\\Jonas\\Zotero\\storage\\DRZA76UW\\Loquercio et al. - 2020 - A General Framework for Uncertainty Estimation in .pdf:application/pdf}
}

@article{osband_deep_2016,
	title = {Deep Exploration via Bootstrapped {DQN}},
	url = {http://arxiv.org/abs/1602.04621},
	abstract = {Eﬃcient exploration remains a major challenge for reinforcement learning ({RL}). Common dithering strategies for exploration, such as -greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically eﬃcient {RL} are not computationally tractable in complex environments. Randomized value functions oﬀer a promising approach to eﬃcient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a ﬁrst step towards addressing such contexts we develop bootstrapped {DQN}. We demonstrate that bootstrapped {DQN} can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped {DQN} substantially improves learning speed and cumulative performance across most games.},
	journaltitle = {{arXiv}:1602.04621 [cs, stat]},
	author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	urldate = {2020-09-26},
	date = {2016-07-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1602.04621},
	keywords = {read, Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Systems and Control},
	file = {Osband et al. - 2016 - Deep Exploration via Bootstrapped DQN.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\5Z9Q9QFY\\Osband et al. - 2016 - Deep Exploration via Bootstrapped DQN.pdf:application/pdf}
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	pages = {529--533},
	number = {7540},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	urldate = {2020-10-23},
	date = {2015-02},
	langid = {english},
	keywords = {read},
	file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:C\:\\Users\\Jonas\\Zotero\\storage\\D3FITC3U\\Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf}
}

@online{openai_gym_nodate,
	title = {Gym: A toolkit for developing and comparing reinforcement learning algorithms},
	url = {https://gym.openai.com},
	shorttitle = {Gym},
	abstract = {A toolkit for developing and comparing reinforcement learning algorithms},
	author = {{OpenAI}},
	urldate = {2020-11-17},
	file = {Snapshot:C\:\\Users\\Jonas\\Zotero\\storage\\DR8DN6XW\\gym.openai.com.html:text/html}
}